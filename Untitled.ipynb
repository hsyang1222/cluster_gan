{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43375a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.9.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Using generated real image info.\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(32, 32), interpolation=bilinear)\n",
      "               CenterCrop(size=(32, 32))\n",
      "               ToTensor()\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "import generative_model_score\n",
    "score_model = generative_model_score.GenerativeModelScore()\n",
    "score_model.lazy_mode(True)\n",
    "import dataset\n",
    "train_loader = dataset.get_cifar1_dataset(2048, 32, shuffle=False)\n",
    "score_model.load_or_make(train_loader)\n",
    "\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "865193b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = iter(train_loader).next()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82461673",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990f67be",
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo vi /etc/ssh/sshd_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7cc0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo service ssh restart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df9f170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b93c6de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "53a4d8cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': Dataset CIFAR10\n",
       "     Number of datapoints: 50000\n",
       "     Root location: data\n",
       "     Split: Train\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                Resize(size=(32, 32), interpolation=bilinear)\n",
       "                CenterCrop(size=(32, 32))\n",
       "                ToTensor()\n",
       "            ),\n",
       " 'num_workers': 0,\n",
       " 'prefetch_factor': 2,\n",
       " 'pin_memory': False,\n",
       " 'timeout': 0,\n",
       " 'worker_init_fn': None,\n",
       " '_DataLoader__multiprocessing_context': None,\n",
       " '_dataset_kind': 0,\n",
       " 'batch_size': 2048,\n",
       " 'drop_last': False,\n",
       " 'sampler': <torch.utils.data.sampler.RandomSampler at 0x7f59f60dd8e0>,\n",
       " 'batch_sampler': <torch.utils.data.sampler.BatchSampler at 0x7f59f5b001f0>,\n",
       " 'generator': None,\n",
       " 'collate_fn': <function torch.utils.data._utils.collate.default_collate(batch)>,\n",
       " 'persistent_workers': False,\n",
       " '_DataLoader__initialized': True,\n",
       " '_IterableDataset_len_called': None,\n",
       " '_iterator': None}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e61a55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1771017c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning /root/.cache/pykeops-1.5-cpython-39/...\n",
      "    - /root/.cache/pykeops-1.5-cpython-39/build-cbfb8499a8 has been removed.\n",
      "    - /root/.cache/pykeops-1.5-cpython-39/keops_hash.log has been removed.\n",
      "    - /root/.cache/pykeops-1.5-cpython-39/build-pybind11_template-libKeOps_template_39e028f4b7 has been removed.\n",
      "    - /root/.cache/pykeops-1.5-cpython-39/libKeOpstorch128a799c85 has been removed.\n",
      "    - /root/.cache/pykeops-1.5-cpython-39/libKeOpstorchf8cd9b5cfd has been removed.\n",
      "[pyKeOps] Initializing build folder for dtype=float32 and lang=torch in /root/.cache/pykeops-1.5-cpython-39 ... done.\n",
      "[pyKeOps] Compiling libKeOpstorch128a799c85 in /root/.cache/pykeops-1.5-cpython-39:\n",
      "       formula: Sum_Reduction(SqNorm2(x - y),1)\n",
      "       aliases: x = Vi(0,3); y = Vj(1,3); \n",
      "       dtype  : float32\n",
      "... \n",
      "[pyKeOps] Compiling pybind11 template libKeOps_template_39e028f4b7 in /root/.cache/pykeops-1.5-cpython-39 ... done.\n",
      "Done.\n",
      "\n",
      "pyKeOps with torch bindings is working!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pykeops\n",
    "pykeops.clean_pykeops()        \n",
    "pykeops.test_torch_bindings() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c22935a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from pykeops.torch import LazyTensor\n",
    "\n",
    "def KMeans(x, K=10, Niter=10, verbose=True, device='cuda:0'):\n",
    "    \n",
    "    use_cuda = 'cuda' in device\n",
    "    dtype = torch.float32 if use_cuda else torch.float64\n",
    "    \n",
    "    \"\"\"Implements Lloyd's algorithm for the Euclidean metric.\"\"\"\n",
    "\n",
    "    start = time.time()\n",
    "    N, D = x.shape  # Number of samples, dimension of the ambient space\n",
    "\n",
    "    c = x[:K, :].clone()  # Simplistic initialization for the centroids\n",
    "\n",
    "    x_i = LazyTensor(x.view(N, 1, D))  # (N, 1, D) samples\n",
    "    c_j = LazyTensor(c.view(1, K, D))  # (1, K, D) centroids\n",
    "\n",
    "    # K-means loop:\n",
    "    # - x  is the (N, D) point cloud,\n",
    "    # - cl is the (N,) vector of class labels\n",
    "    # - c  is the (K, D) cloud of cluster centroids\n",
    "    for i in range(Niter):\n",
    "\n",
    "        # E step: assign points to the closest cluster -------------------------\n",
    "        D_ij = ((x_i - c_j) ** 2).sum(-1)  # (N, K) symbolic squared distances\n",
    "        cl = D_ij.argmin(dim=1).long().view(-1)  # Points -> Nearest cluster\n",
    "\n",
    "        # M step: update the centroids to the normalized cluster average: ------\n",
    "        # Compute the sum of points per cluster:\n",
    "        c.zero_()\n",
    "        c.scatter_add_(0, cl[:, None].repeat(1, D), x)\n",
    "\n",
    "        # Divide by the number of points per cluster:\n",
    "        Ncl = torch.bincount(cl, minlength=K).type_as(c).view(K, 1)\n",
    "        c /= Ncl  # in-place division to compute the average\n",
    "\n",
    "    if verbose:  # Fancy display -----------------------------------------------\n",
    "        if use_cuda:\n",
    "            torch.cuda.synchronize()\n",
    "        end = time.time()\n",
    "        print(\n",
    "            f\"K-means for the Euclidean metric with {N:,} points in dimension {D:,}, K = {K:,}:\"\n",
    "        )\n",
    "        print(\n",
    "            \"Timing for {} iterations: {:.5f}s = {} x {:.5f}s\\n\".format(\n",
    "                Niter, end - start, Niter, (end - start) / Niter\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return cl, c\n",
    "\n",
    "def KMeans_dataloader(trainloader, K=10, Niter=10, verbose=True, cuda='cuda:0') : \n",
    "    x_list = []\n",
    "    for data, label in trainloader : \n",
    "        x_list.append(data)\n",
    "    \n",
    "    x = torch.cat(x_list).to(cuda)\n",
    "    KMeans(x, K, Niter, verbose)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "447079fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_feature_tensor_cuda = torch.tensor(score_model.real_feature_np, device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66f2ad2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pyKeOps] Compiling libKeOpstorchf8cd9b5cfd in /root/.cache/pykeops-1.5-cpython-39:\n",
      "       formula: ArgMin_Reduction(Sum(Square((Var(0,2048,0) - Var(1,2048,1)))),0)\n",
      "       aliases: Var(0,2048,0); Var(1,2048,1); \n",
      "       dtype  : float32\n",
      "... \n",
      "Done.\n",
      "K-means for the Euclidean metric with 50,000 points in dimension 2,048, K = 100:\n",
      "Timing for 100 iterations: 50.54922s = 100 x 0.50549s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "K = 100\n",
    "cl, c = KMeans(real_feature_tensor_cuda, K, Niter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72c14e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "numdata_each_cluster = torch.bincount(cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b916452b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "ndf = 64\n",
    "ngf = 64\n",
    "nz = 100\n",
    "nc = 3\n",
    "ngpu = 1\n",
    "\n",
    "class Assigner(nn.Module):\n",
    "    def __init__(self, ngpu, K):\n",
    "        super(Assigner, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Flatten(),\n",
    "            #  -- original code --\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            #nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            #nn.Conv2d(ndf * 8, 1, 2, 1, 0, bias=False),\n",
    "            # state size. 1x1x1\n",
    "            #nn.Sigmoid()\n",
    "            nn.Linear(2048, K)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        if input.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "\n",
    "        return output.view(-1, 1).squeeze(1)\n",
    "\n",
    "assign = Assigner(ngpu, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8495d8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f5e8c432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b7e955",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b96faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "\n",
    "    parser.add_argument('--device', type=str, default='cuda')\n",
    "    parser.add_argument('--epochs', type=int, default=100)\n",
    "    parser.add_argument('--batch_size', type=int, default=64)\n",
    "    parser.add_argument('--img_size', type=int, default=32)\n",
    "    parser.add_argument('--save_image_interval', type=int, default=5)\n",
    "    parser.add_argument('--loss_calculation_interval', type=int, default=5)\n",
    "    parser.add_argument('--latent_dim', type=int, default=10)\n",
    "    parser.add_argument('--n_iter', type=int, default=3)\n",
    "    parser.add_argument('--project_name', type=str, default='AAE')\n",
    "    parser.add_argument('--dataset', type=str, default='', choices=['LSUN_dining_room', 'LSUN_classroom', 'LSUN_conference', 'LSUN_churches',\n",
    "                                                                    'FFHQ', 'CelebA', 'cifar10', 'mnist', 'mnist_fashion', 'emnist'])\n",
    "\n",
    "    parser.add_argument('--model_name', type=str, default='', choices=['vanilla', 'yeop_loss', \n",
    "                                                                       'yeop_n_iter', 'mod_var', 'mod2_var', 'gme', 'latent_mapping', 'gme_inference', 'direct'])\n",
    "\n",
    "    parser.add_argument('--lr', type=float, default=1e-4)\n",
    "    parser.add_argument('--run_test', type=bool, default=False)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stylegan2",
   "language": "python",
   "name": "stylegan2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
